import streamlit as st
from langchain_community.llms import HuggingFaceHub
from langchain.prompts import ChatPromptTemplate
from langchain.chains import LLMChain
import os

# --- Configuraci√≥n de la p√°gina ---
st.set_page_config(
    page_title=" üå± Asistente Agr√≠cola con IA (Hugging Face)",
    page_icon="üë®‚Äçüåæ",
    layout="centered",
    initial_sidebar_state="auto"
)

st.title("üå± Asistente Agr√≠cola con IA (Hugging Face)")
st.write("¬°Hola! Soy tu asistente agr√≠cola impulsado por IA, ahora desde la nube con Hugging Face. "
         "Preg√∫ntame sobre cultivos, plagas, suelos, fertilizantes y m√°s.")

# --- Configuraci√≥n de Hugging Face ---
# **ADVERTENCIA DE SEGURIDAD:**
# Nunca incluyas tu token directamente en el c√≥digo fuente en un repositorio p√∫blico.
# Para despliegues reales, usa os.environ.get("HUGGINGFACEHUB_API_TOKEN")
# o los secretos de Streamlit (st.secrets["HUGGINGFACEHUB_API_TOKEN"]).
# Aqu√≠ lo incluimos directamente para fines de prueba y demostraci√≥n.
HUGGINGFACEHUB_API_TOKEN = "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"
os.environ["HUGGINGFACEHUB_API_TOKEN"] = HUGGINGFACEHUB_API_TOKEN

# Modelos recomendados y GRATUITOS de Hugging Face para LangChain:
# Puedes buscar m√°s en Hugging Face Hub que sean compatibles con text-generation.
# Modelos buenos y relativamente peque√±os para empezar (pueden ser lentos en la versi√≥n gratuita):
# - google/flan-t5-xxl (grande, pero un buen ejemplo de LLM)
# - google/flan-t5-base (m√°s peque√±o y r√°pido)
# - gpt2 (muy b√°sico, solo para probar la conexi√≥n)
# - microsoft/DialoGPT-medium (dise√±ado para chat, pero no para instrucciones complejas)

# Recomendaci√≥n: Empieza con un modelo como 'google/flan-t5-base' o 'HuggingFaceH4/zephyr-7b-beta'
# (este √∫ltimo puede ser m√°s lento si no tienes acceso a inferencia acelerada)
# Para este ejemplo, usaremos 'google/flan-t5-base' por su disponibilidad general y tama√±o manejable.
repo_id = "google/flan-t5-base"

try:
    llm = HuggingFaceHub(
        repo_id=repo_id,
        model_kwargs={"temperature": 0.5, "max_length": 500} # Ajusta la temperatura y la longitud m√°xima
    )
except Exception as e:
    st.error(f"Error al conectar con Hugging Face Hub. Aseg√∫rate de que tu token es v√°lido "
             f"y el modelo '{repo_id}' existe y es accesible. Error: {e}")
    st.stop()

# Plantilla de prompt para el asistente agr√≠cola
template = """
Eres un asistente de inteligencia artificial experto en agricultura. Tu objetivo es proporcionar informaci√≥n precisa, √∫til y pr√°ctica sobre una amplia gama de temas agr√≠colas. Responde a las preguntas de los usuarios de manera clara, concisa y f√°cil de entender.

Temas que puedes cubrir:
- Cultivos (siembra, cuidado, cosecha, enfermedades, etc.)
- Plagas y enfermedades (identificaci√≥n, prevenci√≥n, tratamiento)
- Suelos (tipos, an√°lisis, mejora, nutrientes)
- Fertilizaci√≥n (tipos de fertilizantes, cu√°ndo y c√≥mo aplicarlos)
- Riego (m√©todos, necesidades de agua por cultivo)
- Ganader√≠a (conceptos b√°sicos, salud animal, alimentaci√≥n, etc. - si el contexto lo permite)
- Maquinaria agr√≠cola (usos b√°sicos, mantenimiento - si el contexto lo permite)

Si la pregunta no est√° relacionada con la agricultura, por favor, ind√≠calo amablemente y redirige al usuario a temas agr√≠colas.

Pregunta del usuario: {question}
"""

prompt = ChatPromptTemplate.from_template(template)
llm_chain = LLMChain(llm=llm, prompt=prompt)

# --- Interfaz de usuario de Streamlit ---

# Inicializar historial de chat
if "messages" not in st.session_state:
    st.session_state.messages = []

# Mostrar mensajes hist√≥ricos
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# Entrada de usuario
user_question = st.chat_input("Haz tu pregunta sobre agricultura aqu√≠...")

if user_question:
    # A√±adir pregunta del usuario al historial
    st.session_state.messages.append({"role": "user", "content": user_question})
    with st.chat_message("user"):
        st.markdown(user_question)

    with st.chat_message("assistant"):
        with st.spinner("Pensando..."):
            try:
                # Generar respuesta usando LangChain
                response = llm_chain.run(question=user_question)
                st.markdown(response)
                # A√±adir respuesta del asistente al historial
                st.session_state.messages.append({"role": "assistant", "content": response})
            except Exception as e:
                st.error(f"Ocurri√≥ un error al generar la respuesta. Por favor, int√©ntalo de nuevo. Error: {e}")

# --- Informaci√≥n Adicional en Sidebar ---
st.sidebar.header("Acerca de este Asistente")
st.sidebar.info(
    "Este asistente utiliza **Hugging Face Hub** para acceder a modelos de lenguaje grandes (LLMs) "
    "y **LangChain** para la orquestaci√≥n. Es una herramienta experimental y la calidad de las respuestas "
    "depende del modelo LLM subyacente y la calidad del prompt."
)
st.sidebar.write("Desarrollado con ‚ù§Ô∏è para la comunidad agr√≠cola.")

st.sidebar.header("Consideraciones de Hugging Face")
st.sidebar.markdown(
    """
    1.  **Token de API:** Aseg√∫rate de que tu token de Hugging Face (`HF_TOKEN` o `HUGGINGFACEHUB_API_TOKEN`) est√© configurado correctamente.
    2.  **Modelos Gratuitos:** La inferencia gratuita en Hugging Face puede ser lenta o tener l√≠mites de uso. Considera modelos m√°s peque√±os para mejor rendimiento.
    3.  **Tipo de Modelo:** El modelo (`repo_id`) debe ser apto para "text generation" o "conversational" para funcionar bien como asistente. `google/flan-t5-base` es un buen punto de partida.
    """
)
