import streamlit as st
from langchain_community.llms import Ollama
from langchain.prompts import ChatPromptTemplate
from langchain.chains import LLMChain

# --- Configuraci√≥n de la p√°gina ---
st.set_page_config(
    page_title=" üå± Asistente Agr√≠cola con IA",
    page_icon="üë®‚Äçüåæ",
    layout="centered",
    initial_sidebar_state="auto"
)

st.title("üå± Asistente Agr√≠cola con IA")
st.write("¬°Hola! Soy tu asistente agr√≠cola impulsado por IA. Preg√∫ntame sobre cultivos, plagas, suelos, fertilizantes y m√°s. "
         "Estoy aqu√≠ para ayudarte a optimizar tus pr√°cticas agr√≠colas.")

# --- Configuraci√≥n de Ollama y LangChain ---
# Aseg√∫rate de que Ollama est√© corriendo y de que hayas descargado un modelo como 'llama2' o 'mistral'.
# Puedes cambiar el modelo aqu√≠ si tienes otro descargado.
try:
    llm = Ollama(model="llama2") # O "mistral" si prefieres ese modelo
except Exception as e:
    st.error(f"Error al conectar con Ollama. Aseg√∫rate de que Ollama est√© ejecut√°ndose y el modelo 'llama2' "
             f"(o el que hayas especificado) est√© descargado. Error: {e}")
    st.stop() # Detiene la ejecuci√≥n si no se puede conectar con Ollama

# Plantilla de prompt para el asistente agr√≠cola
# Es crucial una buena ingenier√≠a de prompts para obtener respuestas relevantes.
template = """
Eres un asistente de inteligencia artificial experto en agricultura. Tu objetivo es proporcionar informaci√≥n precisa, √∫til y pr√°ctica sobre una amplia gama de temas agr√≠colas. Responde a las preguntas de los usuarios de manera clara, concisa y f√°cil de entender.

Temas que puedes cubrir:
- Cultivos (siembra, cuidado, cosecha, enfermedades, etc.)
- Plagas y enfermedades (identificaci√≥n, prevenci√≥n, tratamiento)
- Suelos (tipos, an√°lisis, mejora, nutrientes)
- Fertilizaci√≥n (tipos de fertilizantes, cu√°ndo y c√≥mo aplicarlos)
- Riego (m√©todos, necesidades de agua por cultivo)
- Ganader√≠a (conceptos b√°sicos, salud animal, alimentaci√≥n, etc. - si el contexto lo permite)
- Maquinaria agr√≠cola (usos b√°sicos, mantenimiento - si el contexto lo permite)

Si la pregunta no est√° relacionada con la agricultura, por favor, ind√≠calo amablemente y redirige al usuario a temas agr√≠colas.

Pregunta del usuario: {question}
"""

prompt = ChatPromptTemplate.from_template(template)
llm_chain = LLMChain(llm=llm, prompt=prompt)

# --- Interfaz de usuario de Streamlit ---

# Inicializar historial de chat
if "messages" not in st.session_state:
    st.session_state.messages = []

# Mostrar mensajes hist√≥ricos
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# Entrada de usuario
user_question = st.chat_input("Haz tu pregunta sobre agricultura aqu√≠...")

if user_question:
    # A√±adir pregunta del usuario al historial
    st.session_state.messages.append({"role": "user", "content": user_question})
    with st.chat_message("user"):
        st.markdown(user_question)

    with st.chat_message("assistant"):
        with st.spinner("Pensando..."):
            try:
                # Generar respuesta usando LangChain
                response = llm_chain.run(question=user_question)
                st.markdown(response)
                # A√±adir respuesta del asistente al historial
                st.session_state.messages.append({"role": "assistant", "content": response})
            except Exception as e:
                st.error(f"Ocurri√≥ un error al generar la respuesta. Por favor, int√©ntalo de nuevo. Error: {e}")

# --- Informaci√≥n Adicional en Sidebar ---
st.sidebar.header("Acerca de este Asistente")
st.sidebar.info(
    "Este asistente utiliza **Ollama** para ejecutar modelos de lenguaje grandes de forma local y **LangChain** "
    "para la orquestaci√≥n. Es una herramienta experimental y la calidad de las respuestas depende del modelo LLM "
    "subyacente y la calidad del prompt."
)
st.sidebar.write("Desarrollado con ‚ù§Ô∏è para la comunidad agr√≠cola.")

st.sidebar.header("Pasos para usar Ollama")
st.sidebar.markdown(
    """
    1.  **Descarga e instala Ollama:** Visita [ollama.com](https://ollama.com/)
    2.  **Descarga un modelo:** Abre tu terminal y ejecuta `ollama run llama2` o `ollama run mistral`. Esto descargar√° el modelo y lo iniciar√°. Aseg√∫rate de que el nombre del modelo coincida con el `model` configurado en `main.py`.
    3.  **Ejecuta Streamlit:** Navega al directorio de tu proyecto en la terminal y ejecuta `streamlit run main.py`.
    """
)
